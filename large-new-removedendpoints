import os
import uuid
import logging
import traceback
from typing import Optional, Dict, List
import torch
import torchaudio
import librosa
import soundfile as sf
import numpy as np
import tempfile
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from transformers import WhisperForConditionalGeneration, WhisperProcessor, AutoModel
from contextlib import asynccontextmanager
import re
from pydub import AudioSegment
from pydub import silence  

# Import configuration
try:
    from config import Config
except ImportError:
    from .config import Config

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Global variables for models
whisper_model = None
whisper_processor = None
indic_model = None
models_loading = False
models_loaded = False

# Use configuration values
WHISPER_TO_INDIC_MAPPING = Config.WHISPER_TO_INDIC_MAPPING
SUPPORTED_FORMATS = Config.SUPPORTED_FORMATS

# ------------ Helper: Check model loaded status ------------
def check_all_models_loaded() -> Dict[str, object]:
    return {
        "whisper_model_loaded": whisper_model is not None,
        "whisper_processor_loaded": whisper_processor is not None,
        "indic_model_loaded": indic_model is not None,
        "all_models_ready": all([
            whisper_model is not None,
            whisper_processor is not None,
            indic_model is not None
        ]),
        "models_loading": models_loading,
        "device": Config.get_device(),
        "gpu_available": torch.cuda.is_available()
    }

# ------------ Lifespan event for model loading ------------
@asynccontextmanager
async def lifespan(app: FastAPI):
    global models_loading
    logger.info("Starting Multi-Indic Speech-to-Text Service")
    logger.info(f"Device: {Config.get_device()}")
    logger.info(f"Using Whisper model: {Config.WHISPER_MODEL}")
    logger.info(f"Using Indic model: {Config.INDIC_MODEL}")
    await load_models_async()
    yield
    logger.info("Shutting down Multi-Indic Speech-to-Text Service")

app = FastAPI(
    title="Multi-Indic Language Speech-to-Text API",
    description="Advanced FastAPI service for transcribing audio in 22+ Indian languages with native script output",
    version="2.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ------------ Model loading ------------
async def load_models_async():
    global whisper_model, whisper_processor, indic_model, models_loading, models_loaded
    models_loading = True
    try:
        logger.info("Loading Multi-Indic Language Models...")
        # Whisper Processor
        processor_result = WhisperProcessor.from_pretrained(
            Config.WHISPER_MODEL,
            **Config.get_model_kwargs()
        )
        if isinstance(processor_result, tuple):
            whisper_processor = processor_result[0]
        else:
            whisper_processor = processor_result
        logger.info("Whisper processor loaded successfully from local cache")
        # Whisper Model
        whisper_model = WhisperForConditionalGeneration.from_pretrained(
            Config.WHISPER_MODEL,
            **Config.get_model_kwargs(),
            device_map="auto" if Config.get_device() == "cuda" else None
        )
        logger.info("Whisper model loaded successfully from local cache")
        # Indic model
        try:
            indic_model = AutoModel.from_pretrained(
                Config.INDIC_MODEL,
                trust_remote_code=True,
                cache_dir=Config.MODEL_CACHE_DIR,
                local_files_only=Config.USE_OFFLINE_MODELS
            )
            logger.info("Indic-Conformer loaded successfully")
        except Exception as ind_err:
            logger.warning(f"Indic-Conformer not available offline: {ind_err}")
            indic_model = None
        models_loaded = True
        logger.info("All models ready (offline mode)")
    except Exception as e:
        logger.error(f"Model load failed: {e}")
        logger.error(traceback.format_exc())
        models_loaded = False
    finally:
        models_loading = False

def require_whisper_model() -> bool:
    if whisper_model is None:
        logger.warning("Whisper model not loaded yet.")
        return False
    return True

def require_whisper_processor() -> bool:
    if whisper_processor is None:
        logger.warning("Whisper processor not loaded yet.")
        return False
    return True

def require_indic_model() -> bool:
    if indic_model is None:
        logger.warning("Indic-Conformer model not loaded yet.")
        return False
    return True

# ------------ Helper: detect_audio_segments ------------
def detect_audio_segments(audio_array: np.ndarray, sampling_rate: int) -> List[dict]:
    """Return a list of segments split on silence."""
    if len(audio_array) == 0:
        logger.warning("Empty audio array provided to detect_audio_segments")
        return [{
            "audio": audio_array,
            "start_time": 0.0,
            "end_time": 0.0,
            "segment_id": 0
        }]
    try:
        audio_seg = AudioSegment(
            audio_array.tobytes(),
            frame_rate=sampling_rate,
            sample_width=audio_array.dtype.itemsize,
            channels=1
        )
        chunks = silence.split_on_silence(
            audio_seg,
            min_silence_len=Config.MIN_SILENCE_DURATION,
            silence_thresh=Config.SILENCE_THRESHOLD,
            keep_silence=Config.KEEP_SILENCE_PADDING
        )
        segments, cursor = [], 0
        for idx, ch in enumerate(chunks):
            if not isinstance(ch, AudioSegment):
                logger.warning(f"Chunk at index {idx} is not an AudioSegment, skipping.")
                continue
            if len(ch) < Config.MIN_SEGMENT_DURATION:
                cursor += len(ch)
                continue
            arr = np.array(ch.get_array_of_samples(), dtype=np.float32)
            if ch.sample_width == 2:
                arr /= 32768.0
            segments.append({
                "audio": arr,
                "start_time": cursor / 1000.0,
                "end_time": (cursor + len(ch)) / 1000.0,
                "segment_id": idx,
                "duration": len(ch) / 1000.0
            })
            cursor += len(ch)
        logger.info(f"Audio segmentation complete: {len(segments)} segments detected")
        return segments or [{
            "audio": audio_array,
            "start_time": 0.0,
            "end_time": len(audio_array) / sampling_rate,
            "segment_id": 0,
            "duration": len(audio_array) / sampling_rate
        }]
    except Exception as e:
        logger.error(f"Error in audio segment detection: {str(e)}")
        return [{
            "audio": audio_array,
            "start_time": 0.0,
            "end_time": len(audio_array) / sampling_rate,
            "segment_id": 0,
            "duration": len(audio_array) / sampling_rate
        }]

# ------------ More helpers (unchanged, from your code) ------------

def validate_audio_file(file: UploadFile) -> bool:
    if not file.filename:
        return False
    if hasattr(file, 'size') and file.size is not None and file.size > Config.MAX_FILE_SIZE:
        return False
    file_extension = os.path.splitext(file.filename.lower())[1]
    return file_extension in SUPPORTED_FORMATS

def preprocess_audio(audio_path: str, target_sr: Optional[int] = None) -> np.ndarray:
    if target_sr is None:
        target_sr = Config.SAMPLE_RATE
    try:
        sr_to_use = int(target_sr) if target_sr is not None else Config.SAMPLE_RATE
        audio, sr = librosa.load(audio_path, sr=sr_to_use, mono=True)
        if len(audio) > 0:
            audio = audio / np.max(np.abs(audio))
        return audio
    except Exception as e:
        logger.error(f"Error preprocessing audio: {str(e)}")
        raise e

def contains_native_script(text: str, lang: str) -> bool:
    patterns = {
        'hi': r'[\u0900-\u097F]', 'bn': r'[\u0980-\u09FF]',
        'ta': r'[\u0B80-\u0BFF]', 'te': r'[\u0C00-\u0C7F]',
        'ml': r'[\u0D00-\u0D7F]', 'gu': r'[\u0A80-\u0AFF]',
        'mr': r'[\u0900-\u097F]', 'kn': r'[\u0C80-\u0CFF]',
        'pa': r'[\u0A00-\u0A7F]', 'or': r'[\u0B00-\u0B7F]',
    }
    return bool(re.search(patterns.get(lang, r'[\u0900-\u097F]'), text))

def _transcribe_one_segment(arr: np.ndarray, lang: Optional[str]) -> dict:
    temp_fd, tmp = tempfile.mkstemp(suffix=".wav")
    os.close(temp_fd)
    sf.write(tmp, arr, Config.SAMPLE_RATE)
    out = transcribe_with_whisper(tmp, lang)
    os.remove(tmp)
    return out

def detect_language_from_output(predicted_ids) -> str:
    if not require_whisper_processor():
        return 'en'
    if whisper_processor is None:
        logger.error("Whisper processor is not loaded (None) at batch_decode in detect_language_from_output.")
        return 'en'
    text = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    language_patterns = {
        'hi': ['अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'क', 'ख', 'ग', 'घ'],
        'bn': ['অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ', 'ক', 'খ', 'গ', 'ঘ'],
        'pa': ['ਅ', 'ਆ', 'ਇ', 'ਈ', 'ਉ', 'ਊ', 'ਕ', 'ਖ', 'ਗ', 'ਘ'],
        'ta': ['அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'க', 'ங', 'ச', 'ஞ'],
        'te': ['అ', 'ఆ', 'ఇ', 'ఈ', 'ఉ', 'ఊ', 'క', 'ఖ', 'గ', 'ఘ'],
        'ml': ['അ', 'ആ', 'ഇ', 'ഈ', 'ഉ', 'ഊ', 'ക', 'ഖ', 'ഗ', 'ഘ'],
        'kn': ['ಅ', 'ಆ', 'ಇ', 'ಈ', 'ಉ', 'ಊ', 'ಕ', 'ಖ', 'ಗ', 'ಘ'],
        'gu': ['અ', 'આ', 'ઇ', 'ઈ', 'ઉ', 'ઊ', 'ક', 'ખ', 'ગ', 'ઘ'],
        'ur': ['ا', 'آ', 'ب', 'پ', 'ت', 'ٹ', 'د', 'ڈ', 'ر', 'ڑ'],
    }
    language_scores = {}
    for lang_code, patterns in language_patterns.items():
        score = sum(1 for char in patterns if char in text)
        if score > 0:
            language_scores[lang_code] = score
    return max(language_scores, key=lambda x: language_scores[x]) if language_scores else 'en'

def get_language_description(language_code: str) -> dict:
    descriptions = {
        'hi': {'native_name': 'हिन्दी', 'english_name': 'Hindi', 
               'description': 'यह हिन्दी भाषा में आपकी आवाज़ को टेक्स्ट में परिवर्तित किया गया है।'},
        'bn': {'native_name': 'বাংলা', 'english_name': 'Bengali',
               'description': 'এটি বাংলা ভাষায় আপনার কণ্ঠস্বরকে টেক্সটে রূপান্তরিত করা হয়েছে।'},
        'pa': {'native_name': 'ਪੰਜਾਬੀ', 'english_name': 'Punjabi',
               'description': 'ਇਹ ਪੰਜਾਬੀ ਭਾਸ਼ਾ ਵਿੱਚ ਤੁਹਾਡੀ ਆਵਾਜ਼ ਨੂੰ ਟੈਕਸਟ ਵਿੱਚ ਬਦਲਿਆ ਗਿਆ ਹੈ।'},
        'ta': {'native_name': 'தமிழ்', 'english_name': 'Tamil',
               'description': 'இது தமிழ் மொழியில் உங்கள் குரலை உரையாக மாற்றப்பட்டுள்ளது।'},
        'te': {'native_name': 'తెలుగు', 'english_name': 'Telugu',
               'description': 'ఇది తెలుగు భాషలో మీ స్వరాన్ని టెక్స్ట్‌గా మార్చబడింది।'},
        'ml': {'native_name': 'മലയാളം', 'english_name': 'Malayalam',
               'description': 'ഇത് മലയാളം ഭാഷയിൽ നിങ്ങളുടെ ശബ്ദത്തെ ടെക്സ്റ്റേക്ക് മാറ്റിയിരിക്കുന്നു।'},
        'kn': {'native_name': 'ಕನ್ನಡ', 'english_name': 'Kannada',
               'description': 'ಇದು ಕನ್ನಡ ಭಾಷೆಯಲ್ಲಿ ನಿಮ್ಮ ಧ್ವನಿಯನ್ನು ಪಠ್ಯಕ್ಕೆ ಪರಿವರ್ತಿಸಲಾಗಿದೆ।'},
        'gu': {'native_name': 'ગુજરાતી', 'english_name': 'Gujarati',
               'description': 'આ ગુજરાતી ભાષામાં તમારા અવાજને ટેક્સ્ટમાં બદલવામાં આવ્યો છે।'},
        'ur': {'native_name': 'اردو', 'english_name': 'Urdu',
               'description': 'یہ اردو زبان میں آپ کی آواز کو متن میں تبدیل کیا گیا ہے۔'},
        'en': {'native_name': 'English', 'english_name': 'English',
               'description': 'This is your voice converted to text in English language.'}
    }
    return descriptions.get(language_code, descriptions['en'])

def transcribe_with_whisper_multi_indic(audio_path: str, force_lang: Optional[str] = None) -> dict:
    audio, sr = librosa.load(audio_path, sr=Config.SAMPLE_RATE, mono=True)
    segments = detect_audio_segments(audio, int(sr))
    texts, langs, conf_sum = [], {}, 0.0
    for seg in segments:
        res = _transcribe_one_segment(seg["audio"], force_lang)
        if res.get("language") == "en" and not force_lang:
            logger.info("English detected, trying Indic language detection...")
            indic_languages_to_try = ['hi', 'bn', 'ta', 'te', 'ml', 'gu', 'mr', 'kn', 'pa', 'or']
            best_result = res
            for lang_code in indic_languages_to_try:
                try:
                    temp_res = _transcribe_one_segment(seg["audio"], lang_code)
                    if (temp_res.get('text') and contains_native_script(temp_res['text'], lang_code) and len(temp_res['text']) > len(best_result['text'])):
                        best_result = temp_res
                        logger.info(f"Better result found for {lang_code}")
                        break
                except Exception as e:
                    continue
            res = best_result
        if res.get("text"):
            stamp = f"[{seg['start_time']:.1f}s–{seg['end_time']:.1f}s]"
            texts.append(f"{stamp} {res['text']}")
            lang = res["language"]
            langs[lang] = langs.get(lang, 0) + 1
            conf_sum += res.get("confidence", 0.8)
    if not texts:
        return res
    primary_lang = max(langs, key=lambda x: langs[x]) if langs else (force_lang if force_lang else "en")
    return {
        "text": "\n".join(texts),
        "language": primary_lang,
        "language_info": get_language_description(primary_lang),
        "confidence": conf_sum / max(1, len(texts)),
        "segments_processed": len(texts),
        "detected_languages": langs
    }

def transcribe_with_whisper(audio_path: str, language: Optional[str] = None) -> dict:
    try:
        if not require_whisper_processor():
            return {
                'text': '',
                'language': 'en',
                'language_info': get_language_description('en'),
                'confidence': 0.0,
                'error': 'Whisper processor is still loading. Please try again in a few moments.'
            }
        if not require_whisper_model():
            return {
                'text': '',
                'language': 'en',
                'language_info': get_language_description('en'),
                'confidence': 0.0,
                'error': 'Whisper model is still loading. Please try again in a few moments.'
            }
        audio_array, sampling_rate = librosa.load(audio_path, sr=Config.SAMPLE_RATE, mono=True)
        if len(audio_array) == 0:
            raise ValueError("Audio array is empty - check audio file format")
        if whisper_processor is None:
            logger.error("Whisper processor is not loaded (None) at input_features.")
            return {
                'text': '',
                'language': 'en',
                'language_info': get_language_description('en'),
                'confidence': 0.0,
                'error': 'Whisper processor is not loaded. Please try again later.'
            }
        input_features = whisper_processor(
            audio_array, 
            sampling_rate=Config.SAMPLE_RATE, 
            return_tensors="pt"
        ).input_features
        if Config.get_device() == "cuda":
            input_features = input_features.to("cuda")
        if whisper_model is None:
            logger.error("Whisper model is not loaded (None) at generate.")
            return {
                'text': '',
                'language': 'en',
                'language_info': get_language_description('en'),
                'confidence': 0.0,
                'error': 'Whisper model is not loaded. Please try again later.'
            }
        if language and language in WHISPER_TO_INDIC_MAPPING:
            forced_decoder_ids = whisper_processor.get_decoder_prompt_ids(
                language=language, 
                task="transcribe"
            )
            predicted_ids = whisper_model.generate(
                input_features, 
                forced_decoder_ids=forced_decoder_ids
            )
            detected_lang = language
        else:
            predicted_ids = whisper_model.generate(input_features)
            detected_lang = detect_language_from_output(predicted_ids)
        transcription = whisper_processor.batch_decode(
            predicted_ids, 
            skip_special_tokens=True
        )[0].strip()
        language_info = get_language_description(detected_lang)
        logger.info(f"Whisper detected language: {detected_lang}")
        logger.info(f"Whisper transcription: {transcription[:100]}...")
        return {
            'text': transcription,
            'language': detected_lang,
            'language_info': language_info,
            'confidence': 0.9
        }
    except Exception as e:
        logger.error(f"Whisper transcription error: {str(e)}")
        return {
            'text': '',
            'language': 'en',
            'language_info': get_language_description('en'),
            'confidence': 0.0,
            'error': str(e)
        }

def transcribe_with_indic(audio: np.ndarray, language: str) -> Optional[str]:
    try:
        if not require_indic_model():
            logger.warning("Indic-Conformer model not loaded")
            return None
        if indic_model is not None and hasattr(indic_model, 'language_masks'):
            if language not in indic_model.language_masks:
                logger.warning(f"Language '{language}' not supported by Indic-Conformer, falling back to Hindi")
                language = 'hi'
        if indic_model is None:
            logger.error("Indic-Conformer model is not loaded (None) when attempting to transcribe.")
            return None
        result = indic_model(audio, language, "ctc")
        if isinstance(result, str):
            return result.strip()
        elif hasattr(result, 'text'):
            return result.text.strip()
        else:
            logger.warning("Unexpected result format from Indic-Conformer")
            return None
    except KeyError as e:
        logger.error(f"Language key error in Indic-Conformer: {str(e)}")
        try:
            if indic_model is not None:
                result = indic_model(audio, 'hi', "ctc")
                return result.text.strip() if hasattr(result, 'text') else str(result).strip()
            else:
                logger.error("Indic-Conformer model is not loaded (None) during fallback transcription.")
                return None
        except Exception as fallback_error:
            logger.error(f"Fallback transcription also failed: {str(fallback_error)}")
            return None
    except Exception as e:
        logger.error(f"Indic-Conformer transcription error: {str(e)}")
        return None

# ------------ MAIN ENDPOINTS ----------------

@app.post("/transcribe")
async def transcribe_audio(
    file: UploadFile = File(...),
    language: Optional[str] = None,
    use_indic: bool = True
):
    if not validate_audio_file(file):
        raise HTTPException(
            status_code=400, 
            detail={
                "error": "Invalid file",
                "supported_formats": list(SUPPORTED_FORMATS),
                "max_size_mb": Config.MAX_FILE_SIZE // (1024 * 1024)
            }
        )
    if language and language not in WHISPER_TO_INDIC_MAPPING:
        raise HTTPException(
            status_code=400,
            detail={
                "error": f"Unsupported language: {language}",
                "supported_languages": list(WHISPER_TO_INDIC_MAPPING.keys())
            }
        )
    temp_dir = tempfile.mkdtemp()
    temp_filename = f"{uuid.uuid4()}_{file.filename}"
    temp_path = os.path.join(temp_dir, temp_filename)
    try:
        content = await file.read()
        if len(content) > Config.MAX_FILE_SIZE:
            raise HTTPException(
                status_code=413,
                detail=f"File too large. Maximum size: {Config.MAX_FILE_SIZE // (1024 * 1024)}MB"
            )
        with open(temp_path, "wb") as f:
            f.write(content)
        logger.info(f"Processing file: {file.filename} ({len(content)} bytes)")
        if language:
            whisper_result = transcribe_with_whisper(temp_path, language)
        else:
            whisper_result = transcribe_with_whisper_multi_indic(temp_path)
        detected_lang = whisper_result['language']
        response_data = {
          "filename": file.filename,
          "file_size_bytes": len(content),
          "processing_config": {
              "sample_rate": Config.SAMPLE_RATE,
              "device": Config.get_device(),
              "requested_language": language,
              "use_indic": use_indic,
              "offline_mode": Config.USE_OFFLINE_MODELS,
              "multi_segment_processing": language is None
           },
          "whisper": whisper_result,
          "indic": None,
          "final_language": detected_lang,
          "language_info": whisper_result.get(
              "language_info",
              get_language_description(detected_lang)
          ),
          "segments_info": {
               "segments_processed": whisper_result.get("segments_processed", 1),
               "detected_languages": whisper_result.get(
                  "detected_languages",
                   { detected_lang: 1 }
            )
          }
        }
        if use_indic and detected_lang in WHISPER_TO_INDIC_MAPPING and require_indic_model():
            audio_array = preprocess_audio(temp_path, Config.SAMPLE_RATE)
            indic_lang = WHISPER_TO_INDIC_MAPPING[detected_lang]
            indic_result = transcribe_with_indic(audio_array, indic_lang)
            response_data["indic"] = {
                "text": indic_result,
                "language": indic_lang
            }
            response_data["final_language"] = indic_lang
        if response_data["indic"] and response_data["indic"]["text"]:
            response_data["best_transcription"] = response_data["indic"]["text"]
            response_data["transcription_source"] = "indic-conformer"
        else:
            response_data["best_transcription"] = response_data["whisper"]["text"]
            response_data["transcription_source"] = "whisper"
        return JSONResponse(content=response_data)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Transcription error: {str(e)}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return JSONResponse(
            status_code=500,
            content={
                "error": "Transcription failed",
                "detail": str(e),
                "filename": file.filename if file else "unknown"
            }
        )
    finally:
        try:
            if os.path.exists(temp_path):
                os.remove(temp_path)
            os.rmdir(temp_dir)
        except Exception as cleanup_error:
            logger.warning(f"Cleanup error: {cleanup_error}")

@app.get("/models/status")
async def models_status():
    return {
        "models": check_all_models_loaded(),
        "loading_progress": {
            "whisper_processor": {
                "loaded": whisper_processor is not None,
                "model_name": Config.WHISPER_MODEL
            },
            "whisper_model": {
                "loaded": whisper_model is not None,
                "model_name": Config.WHISPER_MODEL
            },
            "indic_model": {
                "loaded": indic_model is not None,
                "model_name": Config.INDIC_MODEL
            }
        },
        "device_info": {
            "current_device": Config.get_device(),
            "gpu_available": torch.cuda.is_available(),
            "gpu_info": Config.get_gpu_info() if hasattr(Config, 'get_gpu_info') else None
        },
        "offline_config": {
            "offline_mode": Config.USE_OFFLINE_MODELS,
            "cache_directory": Config.MODEL_CACHE_DIR
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
